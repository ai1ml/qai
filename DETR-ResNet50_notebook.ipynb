!mkdir -p src
!pip install -q "sagemaker>=2.214.0" "torch>=2.1" torchvision pycocotools scipy qai-hub-models

#----------- New Cell #

%%writefile src/requirements.txt
qai-hub-models
torch>=2.1
torchvision
pycocotools
scipy

#----------- New Cell #

%%writefile src/train_detr.py
import os, json, urllib.request
from pathlib import Path
import torch, torch.nn as nn, torch.nn.functional as F
from torch.utils.data import DataLoader
from torchvision.datasets import CocoDetection
from torchvision import transforms as T
from scipy.optimize import linear_sum_assignment
from qai_hub_models.models.detr_resnet50 import DETRResNet50

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ---------- Utilities ----------
def box_cxcywh_to_xyxy(x):
    x_c, y_c, w, h = x.unbind(-1)
    return torch.stack([x_c - 0.5*w, y_c - 0.5*h, x_c + 0.5*w, y_c + 0.5*h], dim=-1)

def box_iou(a, b):
    inter_lt = torch.max(a[:, None, :2], b[:, :2])
    inter_rb = torch.min(a[:, None, 2:], b[:, 2:])
    inter_wh = (inter_rb - inter_lt).clamp(min=0)
    inter = inter_wh[..., 0] * inter_wh[..., 1]
    area_a = (a[:, 2]-a[:, 0]) * (a[:, 3]-a[:, 1])
    area_b = (b[:, 2]-b[:, 0]) * (b[:, 3]-b[:, 1])
    union = area_a[:, None] + area_b - inter
    return inter / (union + 1e-6)

def generalized_box_iou(a, b):
    iou = box_iou(a, b)
    enclose_lt = torch.min(a[:, None, :2], b[:, :2])
    enclose_rb = torch.max(a[:, None, 2:], b[:, 2:])
    enclose_wh = (enclose_rb - enclose_lt).clamp(min=0)
    enclose_area = enclose_wh[..., 0] * enclose_wh[..., 1]
    area_a = (a[:, 2]-a[:, 0]) * (a[:, 3]-a[:, 1])
    area_b = (b[:, 2]-b[:, 0]) * (b[:, 3]-b[:, 1])
    inter = iou * (area_a[:, None] + area_b - (area_a[:, None] + area_b - (iou * (area_a[:, None] + area_b))))
    union = area_a[:, None] + area_b - inter
    return iou - (enclose_area - union) / (enclose_area + 1e-6)

# ---------- Matcher and Loss ----------
class HungarianMatcher(torch.nn.Module):
    def __init__(self, cost_class=1.0, cost_bbox=5.0, cost_giou=2.0):
        super().__init__()
        self.cost_class, self.cost_bbox, self.cost_giou = cost_class, cost_bbox, cost_giou

    @torch.no_grad()
    def forward(self, outputs, targets):
        bs, Q, C = outputs["pred_logits"].shape
        probs = outputs["pred_logits"].softmax(-1)
        boxes = outputs["pred_boxes"]
        indices = []
        for b in range(bs):
            tgt_ids = targets[b]["labels"]
            tgt_box = targets[b]["boxes"]
            if tgt_ids.numel() == 0:
                indices.append((torch.empty(0, dtype=torch.long), torch.empty(0, dtype=torch.long)))
                continue
            cost_class = -probs[b][:, tgt_ids]
            cost_bbox = torch.cdist(boxes[b], tgt_box, p=1)
            out_xyxy = box_cxcywh_to_xyxy(boxes[b])
            tgt_xyxy = box_cxcywh_to_xyxy(tgt_box)
            cost_giou = -generalized_box_iou(out_xyxy, tgt_xyxy)
            C = self.cost_class*cost_class + self.cost_bbox*cost_bbox + self.cost_giou*cost_giou
            q_ind, t_ind = linear_sum_assignment(C.cpu())
            indices.append((torch.as_tensor(q_ind, dtype=torch.long), torch.as_tensor(t_ind, dtype=torch.long)))
        return indices

class SetCriterion(torch.nn.Module):
    def __init__(self, num_classes, matcher, eos_coef=0.1):
        super().__init__()
        self.num_classes = num_classes
        self.matcher = matcher
        empty = torch.ones(num_classes + 1); empty[-1] = eos_coef
        self.register_buffer("empty_weight", empty)

    def forward(self, outputs, targets):
        indices = self.matcher(outputs, targets)
        src_logits = outputs["pred_logits"]
        B, Q, _ = src_logits.shape
        target_classes = torch.full((B, Q), self.num_classes, dtype=torch.long, device=device)
        for b, (src, tgt) in enumerate(indices):
            if len(src): target_classes[b, src] = targets[b]["labels"][tgt]
        loss_ce = F.cross_entropy(src_logits.transpose(1,2), target_classes, self.empty_weight)
        src_boxes, tgt_boxes = [], []
        for b, (src, tgt) in enumerate(indices):
            if len(src):
                src_boxes.append(outputs["pred_boxes"][b][src])
                tgt_boxes.append(targets[b]["boxes"][tgt])
        if not src_boxes:
            loss_bbox = loss_giou = torch.tensor(0., device=device)
        else:
            src_boxes = torch.cat(src_boxes); tgt_boxes = torch.cat(tgt_boxes)
            loss_bbox = F.l1_loss(src_boxes, tgt_boxes)
            loss_giou = (1 - generalized_box_iou(
                box_cxcywh_to_xyxy(src_boxes), box_cxcywh_to_xyxy(tgt_boxes)
            ).diagonal().mean())
        loss = loss_ce + 5*loss_bbox + 2*loss_giou
        return {"loss_total": loss, "loss_ce": loss_ce, "loss_bbox": loss_bbox, "loss_giou": loss_giou}

# ---------- Data and Target conversion ----------
def coco_targets(targets_coco, images, num_classes):
    outs = []
    for ann, img in zip(targets_coco, images):
        h, w = img.shape[-2:]
        boxes, labels = [], []
        for o in ann:
            x, y, bw, bh = o["bbox"]
            if bw <= 0 or bh <= 0: continue
            cx, cy, ww, hh = (x + bw/2)/w, (y + bh/2)/h, bw/w, bh/h
            boxes.append([cx, cy, ww, hh])
            labels.append(min(o["category_id"], num_classes-1))
        outs.append({
            "boxes": torch.tensor(boxes, dtype=torch.float32, device=device),
            "labels": torch.tensor(labels, dtype=torch.long, device=device)
        })
    return outs

def make_loaders(train_dir, val_dir, bs):
    tfm = T.Compose([T.ToTensor()])
    train = CocoDetection(os.path.join(train_dir, "images"),
                          os.path.join(train_dir, "annotations", "instances_train.json"), transform=tfm)
    val   = CocoDetection(os.path.join(val_dir, "images"),
                          os.path.join(val_dir, "annotations", "instances_val.json"), transform=tfm)
    collate = lambda x: tuple(zip(*x))
    return (DataLoader(train, batch_size=bs, shuffle=True, num_workers=4, collate_fn=collate),
            DataLoader(val, batch_size=bs, shuffle=False, num_workers=4, collate_fn=collate))

# ---------- Optional FB weights ----------
def load_fb_weights(model, path="detr-r50-e632da11.pth"):
    url = "https://dl.fbaipublicfiles.com/detr/detr-r50-e632da11.pth"
    if not Path(path).exists():
        print("Downloading FB pretrained weights...")
        urllib.request.urlretrieve(url, path)
    state = torch.load(path, map_location="cpu")
    model.load_state_dict(state, strict=False)
    print("FB COCO weights loaded (partial).")

# ---------- Main ----------
def main():
    train_dir = os.environ.get("SM_CHANNEL_TRAIN", "/opt/ml/input/data/train")
    val_dir   = os.environ.get("SM_CHANNEL_VAL",   "/opt/ml/input/data/val")
    model_dir = os.environ.get("SM_MODEL_DIR",     "/opt/ml/model")
    os.makedirs(model_dir, exist_ok=True)

    hps = json.loads(os.environ.get("SM_HPS", "{}"))
    num_classes = int(hps.get("num_classes", 5))
    epochs = int(hps.get("epochs", 10))
    lr = float(hps.get("lr", 1e-4))
    bs = int(hps.get("batch_size", 4))
    warmstart = hps.get("warmstart", "true").lower() == "true"
    img_size = int(hps.get("image_size", 800))

    model = DETRResNet50()
    if warmstart: load_fb_weights(model)
    if hasattr(model, "class_embed"):
        in_f = model.class_embed.in_features
        model.class_embed = nn.Linear(in_f, num_classes + 1)
    model.to(device)

    train_loader, val_loader = make_loaders(train_dir, val_dir, bs)
    matcher = HungarianMatcher(); criterion = SetCriterion(num_classes, matcher).to(device)
    optim = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)

    model.train()
    for ep in range(1, epochs+1):
        total = 0; steps = 0
        for imgs, anns in train_loader:
            imgs = [i.to(device) for i in imgs]
            targets = coco_targets(anns, imgs, num_classes)
            out = model(imgs)
            losses = criterion(out, targets)
            loss = losses["loss_total"]
            optim.zero_grad(); loss.backward(); optim.step()
            total += loss.item(); steps += 1
        print(f"Epoch {ep}/{epochs} - loss {total/max(1,steps):.4f}")

    # Save weights
    torch.save(model.state_dict(), os.path.join(model_dir, "checkpoint.pt"))
    # Export ONNX
    dummy = torch.randn(1, 3, img_size, img_size, device=device)
    torch.onnx.export(model, dummy, os.path.join(model_dir, "detr_resnet50.onnx"),
                      input_names=["images"], output_names=["pred_logits","pred_boxes"], opset_version=13)

if __name__ == "__main__":
    main()


#----------- New Cell #

from sagemaker.pytorch import PyTorch
import sagemaker

role = sagemaker.get_execution_role()

# point to your COCO-style data in S3
inputs = {
    "train": "s3://YOUR-BUCKET/my_dataset/train",
    "val":   "s3://YOUR-BUCKET/my_dataset/val"
}

estimator = PyTorch(
    entry_point="train_detr.py",
    source_dir="src",
    role=role,
    instance_type="ml.g5.2xlarge",
    instance_count=1,
    framework_version="2.1",
    py_version="py310",
    hyperparameters={
        "num_classes": 5,
        "epochs": 10,
        "batch_size": 4,
        "lr": 1e-4,
        "warmstart": "true",
        "image_size": 800
    },
    enable_sagemaker_metrics=True,
)

estimator.fit(inputs)





