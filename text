Demo 1 — Computer Vision on IoT with SageMaker + Qualcomm AI Hub
Objective: Demonstrate how AWS SageMaker and Qualcomm AI Hub together enable a seamless, cloud-to-edge AI workflow—where developers can fine-tune a vision model in SageMaker, optimize it automatically in AI Hub, and deploy it on the Qualcomm RB3 Gen2 IoT board for real-time, low-latency inference. The demo highlights how this joint platform accelerates AI innovation from development to device.
1.	Edge AI Value
Explain why running AI locally matters — real-time decisions, lower latency and cost, improved privacy, and continuous operation without cloud dependency.
2.	Computer Vision Use Case
Position image classification as a practical edge workload for retail, logistics, and industrial safety.
Demonstrate how devices identify objects directly from sensor or camera inputs.
3.	Hardware Showcase – Qualcomm RB3 Gen2
Purpose-built for computer-vision and sensor-fusion tasks, supporting multiple Qualcomm SoCs.
4.	Model Selection in SageMaker JumpStart
In SageMaker Studio, open JumpStart → Image Classifier (e.g., MobileNetV2).
Pre-trained models and automated pipelines reduce setup time to minutes.
5.	Fine-Tune and Validate on SageMaker
Fine-tune with a small labeled dataset; monitor metrics in real time.
Demonstrate accuracy and inference checks directly in the notebook — minimal code, fully managed.
6.	Optimize with Qualcomm AI Hub
Export the trained model to AI Hub using a persistent API key.
Apply quantization and hardware-aware conversion (QNN / TFLite / ONNX).
Show compression and latency drop.
7.	Deploy to RB3 via Python SDK
Deploy the optimized model to the RB3 Gen2 device for on-device inference.
Highlight cross-device portability — same flow scales to other Qualcomm edge boards.
8.	Live Inference Demo
Run real-time classification on physical objects
Compare pre- and post-optimization performance (Need to be confirmed) 
9.	Scale for Enterprise Deployment
Show how SageMaker manages datasets and training jobs while AI Hub automates edge deployment.
10.	Outcome and Takeaway
•	Cloud-to-edge workflow completed in hours not weeks.
•	Model size reduced > XX %, inference latency < XXX ms.
•	Demonstrates AWS + Qualcomm + Deloitte delivering scalable, production-ready edge AI.
